# âœ… Checklist estructurado para proyectos de Machine Learning

## ğŸ” 1. Entendimiento del problema
- [ ] Â¿CuÃ¡l es el objetivo del modelo?
- [ ] Â¿CuÃ¡l es la variable objetivo (target)?
- [ ] Â¿QuÃ© mÃ©trica es mÃ¡s importante? (`accuracy`, `f1`, `recall`, etc.)
    Â¿QuÃ© mÃ©trica es mÃ¡s importante?
        Dependiendo del objetivo, las mÃ©tricas pueden cambiar. Algunas opciones son:

        Accuracy: mide cuÃ¡ntas predicciones fueron correctas. Pero no siempre es confiable, especialmente con clases desbalanceadas (por ejemplo, si solo sobrevivieron pocos).

        Precision (precisiÃ³n): Mide cuÃ¡ntos de los predichos como "sobrevivientes" realmente lo fueron. Es importante si no quieres que el modelo haga muchas predicciones incorrectas sobre personas que no sobrevivieron.

        Recall (sensibilidad): Mide cuÃ¡ntos de los sobrevivientes reales fueron identificados por el modelo. Es clave si te interesa no perder sobrevivientes (por ejemplo, en un contexto real, queremos identificar a todos los posibles sobrevivientes, aunque haya mÃ¡s falsos positivos).

        F1-score: Es un balance entre precision y recall. Es Ãºtil si la distribuciÃ³n de clases es desbalanceada y quieres evitar que el modelo se incline demasiado hacia una de las clases.

        ROC-AUC: Mide la habilidad del modelo para diferenciar entre las clases (la curva del "Receiver Operating Characteristic"). Es Ãºtil cuando quieres ver cÃ³mo el modelo clasifica en tÃ©rminos de tasa de verdaderos positivos y falsos positivos.

- [ ] Â¿QuÃ© significan un FP y un FN para el problema?

## ğŸ“Š 2. ExploraciÃ³n del dataset
- [ ] Cargar el dataset crudo
- [ ] Revisar `.info()`, `.describe()`, `.head()`
- [ ] Identificar valores faltantes
- [ ] Observar la distribuciÃ³n de cada variable
- [ ] Verificar si hay clases desbalanceadas

## ğŸ§¹ 3. Preprocesamiento inicial
- [ ] Imputar valores faltantes con lÃ³gica justificada
- [ ] Eliminar o transformar outliers si es necesario
- [ ] Codificar variables categÃ³ricas correctamente
- [ ] Normalizar/escalar si es requerido por el modelo

## ğŸ§  4. Feature Engineering
- [ ] Crear variables nuevas basadas en hipÃ³tesis
- [ ] Generar interacciones Ãºtiles entre columnas
- [ ] Reducir cardinalidad en variables categÃ³ricas si es necesario
- [ ] Documentar cada nueva feature y por quÃ© se creÃ³

## âœ‚ï¸ 5. DivisiÃ³n del dataset
- [ ] Separar en `train` y `test` (o `validation`)
- [ ] Asegurar que la divisiÃ³n sea **estratificada** si hay desbalance

## âš™ï¸ 6. Modelos base
- [ ] Entrenar varios modelos simples sin tuning
- [ ] Comparar con una mÃ©trica elegida
- [ ] Seleccionar 1 o 2 para seguir optimizando

## ğŸ”§ 7. OptimizaciÃ³n de hiperparÃ¡metros
- [ ] Usar `GridSearchCV`, `RandomSearchCV`, etc.
- [ ] Evaluar con validaciÃ³n cruzada
- [ ] Registrar la mejor combinaciÃ³n de hiperparÃ¡metros

## ğŸ“ˆ 8. EvaluaciÃ³n final
- [ ] Medir en test: `accuracy`, `precision`, `recall`, `f1`, `ROC-AUC`
- [ ] Ver matriz de confusiÃ³n
- [ ] Confirmar que el modelo no estÃ¡ sobreajustado

## ğŸ” 9. Interpretabilidad
- [ ] Revisar `feature_importances_` o usar SHAP/LIME
- [ ] Validar que el modelo estÃ© tomando decisiones razonables
- [ ] Documentar interpretaciones clave

## ğŸ§ª 10. Deploy o entrega
- [ ] Armar `submission.csv` o pipeline final
- [ ] Dejar el cÃ³digo limpio y modular
- [ ] Documentar conclusiones, limitaciones y prÃ³ximos pasos
